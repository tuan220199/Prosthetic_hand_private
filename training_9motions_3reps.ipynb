{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from joblib import dump, load\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from data_loader import CustomSignalData, CustomSignalData1\n",
    "from torch.autograd import Variable\n",
    "from encoder import Encoder as E\n",
    "from helpers import set_cmd_cb, rms_formuula, get_data, get_all_data, get_shift_data, get_operators, plot_cfs_mat, roll_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEFINF ESSENTIAL CLASSES, MODULES AND CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cpu\")\n",
    "def getFeatureMatrix(rawDataMatrix, windowLength, windowOverlap):\n",
    "    rms = lambda sig: np.sqrt(np.mean(sig**2))\n",
    "    nChannels,nSamples = rawDataMatrix.shape    \n",
    "    I = int(np.floor(nSamples/(windowLength-windowOverlap)))\n",
    "    featMatrix = np.zeros([nChannels, I])\n",
    "    for channel in range(nChannels):\n",
    "        for i in range (I):\n",
    "            wdwStrtIdx=i*(windowLength-windowOverlap)\n",
    "            sigWin = rawDataMatrix[channel][wdwStrtIdx:(wdwStrtIdx+windowLength-1)] \n",
    "            featMatrix[channel, i] = rms(sigWin)\n",
    "    featMatrixData = np.array(featMatrix)\n",
    "    return featMatrixData\n",
    "\n",
    "class FFNN(torch.nn.Module):\n",
    "    def __init__(self, inputSize, outputSize):\n",
    "        super(FFNN, self).__init__()\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(inputSize, 9, bias=False),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "        self.classifer = torch.nn.Sequential(\n",
    "            torch.nn.Linear(9, outputSize, bias=False),\n",
    "            # torch.nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, encoder=None):\n",
    "        if not encoder:\n",
    "            encoder = self.encoder\n",
    "        z = encoder(x)\n",
    "        class_z = self.classifer(z)\n",
    "\n",
    "        return class_z\n",
    "\n",
    "class Operator(nn.Module):\n",
    "    def __init__(self, in_features, n_rotations):\n",
    "        super(Operator, self).__init__()\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          in_features (int): Number of input features which should be equal to xsize.\n",
    "          out_features (out): Number of output features which should be equal to ysize.\n",
    "        \"\"\"\n",
    "        self.in_features = in_features\n",
    "        self.core = torch.nn.Parameter(torch.zeros(3*self.in_features**2)- 0*torch.diag(torch.rand(3*self.in_features**2)/10))\n",
    "        self.core.requires_grad = True\n",
    "        self.n_rotations = n_rotations\n",
    "        \n",
    "    def rotate_batch(self, x, d, out_features):\n",
    "      rotated = torch.empty(x.shape[0], 3*out_features*out_features, device=DEVICE)\n",
    "      phies = [torch.linalg.matrix_power(self.core,i).to(DEVICE) for i in range (0,self.n_rotations+0)]\n",
    "      for i in range (x.shape[0]):\n",
    "        rotated[i] = phies[(d[i]+0)%4].matmul(x[i]) \n",
    "      return rotated\n",
    "\n",
    "    def forward(self, x, d):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          x of shape (batch_size, 3, xsize, xsize): Inputs.\n",
    "        \n",
    "        Returns:\n",
    "          y of shape (batch_size, 3*xsize^2): Outputs.\n",
    "        \"\"\"\n",
    "        z = self.rotate_batch(x, d, self.in_features)\n",
    "        return z\n",
    "def get_tensor(arr):\n",
    "    return torch.tensor(arr, device=DEVICE,dtype=torch.float )\n",
    "\n",
    "def rotate_batch(x, d, out_features):\n",
    "    M = torch.diag(torch.ones(8)).roll(-1,1)\n",
    "    used_bases = [torch.linalg.matrix_power(M,i).to(DEVICE) for i in range (8)]\n",
    "    rotated = torch.empty(x.shape, device=DEVICE)\n",
    "    for i in range (x.shape[0]):\n",
    "        rotated[i] = used_bases[d[i]].matmul(x[i]) \n",
    "    return rotated\n",
    "\n",
    "def clf_acc(model, loader, masks = None, encoder = None):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    iter = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels,_,_ in loader:\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            if masks is not None:\n",
    "                inputs = inputs * masks[:inputs.size()[0]]\n",
    "            labels = labels.to(DEVICE)\n",
    "            labels = labels.flatten()\n",
    "            if encoder:\n",
    "                pred = model(inputs, encoder)\n",
    "            else:\n",
    "                pred = model(inputs)\n",
    "            correct += (1-torch.abs(torch.sign(torch.argmax(pred,dim = 1)- labels))).mean().item()\n",
    "            iter += 1\n",
    "    return correct/iter\n",
    "\n",
    "def compute_accuracy(a, b, loader):\n",
    "    a.eval()\n",
    "    b.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    iter = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs1, inputs2, shift1, shift2, labels, _ in loader:\n",
    "            inputs1 = inputs1.to(DEVICE)\n",
    "            inputs2 = inputs2.to(DEVICE)\n",
    "            shift1 = -shift1.int().flatten().to(DEVICE)\n",
    "            shift2 = -shift2.int().flatten().to(DEVICE)\n",
    "            labels = labels.flatten().to(DEVICE)\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # forward + backward + optimize\n",
    "            y1 = a(inputs1)\n",
    "            y_tr_est1 = rotate_batch(y1,shift1,6)\n",
    "            y_tr1 = b(y_tr_est1)\n",
    "\n",
    "            y2 = a(inputs2)\n",
    "            y_tr_est2 = rotate_batch(y2,shift1,6)\n",
    "            y_tr2 = b(y_tr_est2)\n",
    "\n",
    "            correct += (1-torch.abs(torch.sign(torch.argmax(y_tr1,dim = 1)- labels))).mean().item() + \\\n",
    "                    (1-torch.abs(torch.sign(torch.argmax(y_tr2,dim = 1)- labels))).mean().item()\n",
    "            iter += 1\n",
    "    return correct * 0.5 / iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOAD DATA AND SEPERATE THE TRAIN AND TEST DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = '26'\n",
    "\n",
    "Fs = 1000\n",
    "windowLength = int(np.floor(0.1*Fs))  #160ms\n",
    "windowOverlap =  int(np.floor(50/100 * windowLength))\n",
    "\n",
    "X_train = [np.zeros([0, 8])] * 3\n",
    "y_train=[np.zeros([0])] * 3\n",
    "X_test = [np.zeros([0, 8])] * 3\n",
    "y_test=[np.zeros([0])] * 3\n",
    "# X_train = np.zeros([0,8])\n",
    "# y_train = np.zeros([0])\n",
    "for k_fold in range(0,3):\n",
    "    for shift in range(0,9): \n",
    "        for files in sorted(os.listdir(f'Subject_{subject}/Shift_{shift}/')):\n",
    "            _, class_,_, rep_ = files.split('_')\n",
    "            if int(class_) in [1,2,3,4,5,6,7,8,9]:\n",
    "                df = pd.read_csv(f'Subject_{subject}/Shift_{shift}/{files}',skiprows=0,sep=' ',header=None)\n",
    "                data_arr = np.stack([np.array(df.T[i::8]).T.flatten().astype('float32') for i in range (8)])\n",
    "                data_arr -= 121\n",
    "                data_arr /= 255.0\n",
    "                feaData = getFeatureMatrix(data_arr, windowLength, windowOverlap)\n",
    "                \n",
    "                if not class_.startswith('9'):\n",
    "                    rms_feature = feaData.sum(0)\n",
    "                    baseline = 2*rms_feature[-50:].mean()\n",
    "                    start_ = np.argmax(rms_feature[::1]>baseline)\n",
    "                    end_  = -np.argmax(rms_feature[::-1]>baseline)\n",
    "                    feaData = feaData.T[start_:end_]\n",
    "                else:\n",
    "                    feaData = feaData.T\n",
    "\n",
    "                if rep_.startswith(str(k_fold+1)):\n",
    "                    X_test[k_fold] = np.concatenate([X_test[k_fold],feaData])\n",
    "                    y_test[k_fold] = np.concatenate([y_test[k_fold],np.ones_like(feaData)[:,0]*int(class_)-1])\n",
    "                else:\n",
    "                    X_train[k_fold] = np.concatenate([X_train[k_fold],feaData])\n",
    "                    y_train[k_fold] = np.concatenate([y_train[k_fold],np.ones_like(feaData)[:,0]*int(class_)-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00800002 0.0031776  0.00327391 ... 0.02204677 0.00692822 0.00452824]\n",
      " [0.00784314 0.0031034  0.00315306 ... 0.02686469 0.00762216 0.00687193]\n",
      " [0.00729944 0.00267314 0.00292296 ... 0.02537185 0.00787279 0.00674646]\n",
      " ...\n",
      " [0.00804841 0.00336747 0.00332102 ... 0.00312833 0.00305294 0.00341329]\n",
      " [0.00771333 0.00315306 0.00320195 ... 0.0031034  0.00302739 0.0034585 ]\n",
      " [0.00760345 0.00357989 0.00311173 ... 0.00311173 0.00282385 0.00353989]]\n",
      "(42004, 8)\n",
      "[0. 0. 0. ... 8. 8. 8.]\n",
      "(42004,)\n",
      "[[0.00899624 0.00365503 0.00305294 ... 0.02290042 0.00879543 0.00738407]\n",
      " [0.01201943 0.00442413 0.00363372 ... 0.03947626 0.01278351 0.01073604]\n",
      " [0.01242612 0.00417111 0.00367622 ... 0.03907481 0.01230047 0.01338891]\n",
      " ...\n",
      " [0.00804841 0.00336747 0.00332102 ... 0.00312833 0.00305294 0.00341329]\n",
      " [0.00771333 0.00315306 0.00320195 ... 0.0031034  0.00302739 0.0034585 ]\n",
      " [0.00760345 0.00357989 0.00311173 ... 0.00311173 0.00282385 0.00353989]]\n",
      "(41977, 8)\n",
      "[0. 0. 0. ... 8. 8. 8.]\n",
      "(41977,)\n",
      "[[0.00899624 0.00365503 0.00305294 ... 0.02290042 0.00879543 0.00738407]\n",
      " [0.01201943 0.00442413 0.00363372 ... 0.03947626 0.01278351 0.01073604]\n",
      " [0.01242612 0.00417111 0.00367622 ... 0.03907481 0.01230047 0.01338891]\n",
      " ...\n",
      " [0.00814435 0.00367622 0.00341329 ... 0.00341329 0.00356902 0.00336747]\n",
      " [0.00785303 0.00329755 0.00305294 ... 0.00302739 0.00307827 0.00341329]\n",
      " [0.00771962 0.00300583 0.00284145 ... 0.00297368 0.00284145 0.00345232]]\n",
      "(41959, 8)\n",
      "[0. 0. 0. ... 8. 8. 8.]\n",
      "(41959,)\n",
      "#############\n",
      "[[0.00899624 0.00365503 0.00305294 ... 0.02290042 0.00879543 0.00738407]\n",
      " [0.01201943 0.00442413 0.00363372 ... 0.03947626 0.01278351 0.01073604]\n",
      " [0.01242612 0.00417111 0.00367622 ... 0.03907481 0.01230047 0.01338891]\n",
      " ...\n",
      " [0.00815388 0.00305294 0.00322611 ... 0.00354719 0.00278694 0.00336747]\n",
      " [0.00743648 0.00252368 0.00305294 ... 0.00320195 0.00236479 0.00334433]\n",
      " [0.00750314 0.002723   0.00314425 ... 0.00291774 0.00245797 0.00343637]]\n",
      "(20966, 8)\n",
      "[0. 0. 0. ... 8. 8. 8.]\n",
      "(20966,)\n",
      "[[0.00800002 0.0031776  0.00327391 ... 0.02204677 0.00692822 0.00452824]\n",
      " [0.00784314 0.0031034  0.00315306 ... 0.02686469 0.00762216 0.00687193]\n",
      " [0.00729944 0.00267314 0.00292296 ... 0.02537185 0.00787279 0.00674646]\n",
      " ...\n",
      " [0.00814435 0.00367622 0.00341329 ... 0.00341329 0.00356902 0.00336747]\n",
      " [0.00785303 0.00329755 0.00305294 ... 0.00302739 0.00307827 0.00341329]\n",
      " [0.00771962 0.00300583 0.00284145 ... 0.00297368 0.00284145 0.00345232]]\n",
      "(20993, 8)\n",
      "[0. 0. 0. ... 8. 8. 8.]\n",
      "(20993,)\n",
      "[[0.0075402  0.00339046 0.00322611 ... 0.02729207 0.00822973 0.00648824]\n",
      " [0.00762216 0.00339046 0.00322611 ... 0.03183215 0.00719224 0.00548963]\n",
      " [0.00842559 0.00327391 0.00332102 ... 0.02981112 0.00766281 0.01000196]\n",
      " ...\n",
      " [0.00804841 0.00336747 0.00332102 ... 0.00312833 0.00305294 0.00341329]\n",
      " [0.00771333 0.00315306 0.00320195 ... 0.0031034  0.00302739 0.0034585 ]\n",
      " [0.00760345 0.00357989 0.00311173 ... 0.00311173 0.00282385 0.00353989]]\n",
      "(21011, 8)\n",
      "[0. 0. 0. ... 8. 8. 8.]\n",
      "(21011,)\n"
     ]
    }
   ],
   "source": [
    "# 3-fold validation for 3 repetitions: \n",
    "# fold 1: train: rep 2, rep 3; test: rep 1\n",
    "# fold 2: train: rep 1, rep 3; test: rep 2\n",
    "# fold 3: train: rep 1, rep 1; test: rep 3\n",
    "for i in range(3):\n",
    "    print(X_train[i])\n",
    "    print(X_train[i].shape)\n",
    "    print(y_train[i])\n",
    "    print(y_train[i].shape)\n",
    "print(\"#############\")\n",
    "for i in range(3):\n",
    "    print(X_test[i])\n",
    "    print(X_test[i].shape)\n",
    "    print(y_test[i])\n",
    "    print(y_test[i].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SIMULATED SHIFT THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader_list = []\n",
    "alltrainloader_list = []\n",
    "triplettrainloader_list = []\n",
    "\n",
    "for k_fold in range(3):\n",
    "    all_X_train, all_y_train, all_shift_train = get_all_data(X_train[k_fold], y_train[k_fold])\n",
    "    all_X1_train, all_X2_train, all_shift_1_train, all_shift_2_train, all_y_shift_train = get_shift_data(all_X_train, all_shift_train, all_y_train)\n",
    "    \n",
    "    traindataset = CustomSignalData(get_tensor(X_train[k_fold]), get_tensor(y_train[k_fold]))\n",
    "    trainloader = torch.utils.data.DataLoader(traindataset, batch_size = 1, shuffle=True)\n",
    "    trainloader_list.append(trainloader)\n",
    "\n",
    "    all_train_dataset = CustomSignalData(get_tensor(all_X_train), get_tensor(all_y_train))\n",
    "    alltrainloader = torch.utils.data.DataLoader(all_train_dataset, batch_size = 102, shuffle=True)\n",
    "    alltrainloader_list.append(alltrainloader)\n",
    "\n",
    "    triplet_train_dataset = CustomSignalData1(get_tensor(all_X1_train), get_tensor(all_X2_train), get_tensor(all_shift_1_train), get_tensor(all_shift_2_train), get_tensor(all_y_shift_train))\n",
    "    triplettrainloader = torch.utils.data.DataLoader(triplet_train_dataset, batch_size = 102, shuffle=True)\n",
    "    triplettrainloader_list.append(triplettrainloader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAIN LOGISTIC REGRESSION LEARNING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "e:\\python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "e:\\python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "for k_fold in range(3):\n",
    "    reg = LogisticRegression(penalty='l2', C=100).fit(X_train[k_fold], y_train[k_fold])\n",
    "    dump(reg, f'LogisticRegression_fold_{k_fold+1}.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST ACCURACY OF LOGISTIC REGRESSION LEARNIGN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 1:\n",
      "[0.3940665839931317, 0.37579891252504055, 0.387150624821139, 0.3919202518363064, 0.41190498902985784, 0.35824668510922447, 0.38295335304779166, 0.35672040446437087]\n",
      "Accuracy of 2:\n",
      "[0.4008479016815129, 0.3988948697184776, 0.37393416853236794, 0.37107607297670653, 0.4197589672748059, 0.3798885342733292, 0.3887486304958796, 0.3550231029390749]\n",
      "Accuracy of 3:\n",
      "[0.4164009328447004, 0.3976012564846985, 0.3930798153348246, 0.3895102565322926, 0.4212079386987768, 0.3919375565180144, 0.3742325448574556, 0.38479843891295035]\n"
     ]
    }
   ],
   "source": [
    "for k_fold in range(3):\n",
    "    logRegres  = load(f'LogisticRegression_fold_{k_fold+1}.joblib')\n",
    "    accuracies_LosReg_shift = []\n",
    "    for i in range (-4, 4):\n",
    "        X_test_shift = roll_data(X_test[k_fold], i)\n",
    "        accuracies_LosReg_shift.append(logRegres.score(X_test_shift,y_test[k_fold]))\n",
    "    print(f'Accuracy of {k_fold+1}:')\n",
    "    print(accuracies_LosReg_shift)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST FEED FORAWRD NEURAL NETWORK LEARNING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6482919255437786, 0.6405415713991671, 0.6659924269609102, 0.6337310675898188, 0.6497493739190855, 0.6349978210655993, 0.6763648250965014, 0.6258921769172703]\n",
      "[0.6602464988231659, 0.6571008402960641, 0.6689859940324511, 0.6611120451859065, 0.6582436982563564, 0.6694649872779846, 0.680263305595943, 0.6748543413025992]\n",
      "[0.709362460734093, 0.6831067526993686, 0.6970345573882534, 0.6867735580154205, 0.6903668542022574, 0.6710945069789886, 0.6962821707469687, 0.6880448321093163]\n"
     ]
    }
   ],
   "source": [
    "accuracies_ffnn_list = []\n",
    "\n",
    "for k_fold in range(3):\n",
    "    modelWOoperator = FFNN(8,9)\n",
    "    modelWOoperator.load_state_dict(torch.load(f\"modelwoOperator_fold_{k_fold+1}.pt\")) # loaded from the file: parameters learned during training.\n",
    "    modelWOoperator.eval() # evaluation mode ensures consistent behavior during inference.\n",
    "\n",
    "\n",
    "    accuracies_ffnn = []\n",
    "    for i in range (-4, 4):\n",
    "        X_test_shift = roll_data(X_test[k_fold], i)\n",
    "        test_shift_dataset = CustomSignalData(get_tensor(X_test_shift), get_tensor(y_test[k_fold]))\n",
    "        testshiftloader = torch.utils.data.DataLoader(test_shift_dataset, batch_size=24, shuffle=True)\n",
    "        accuracies_ffnn.append(clf_acc(modelWOoperator, testshiftloader, encoder = None))\n",
    "\n",
    "    print(accuracies_ffnn)\n",
    "    accuracies_ffnn_list.append(accuracies_ffnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST ACCURACY OF SELF-SUPERVISED LEARNING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0301,  0.1377,  0.4586,  ..., -0.0418,  0.0597,  0.3406],\n",
      "        [ 0.0152,  0.1060, -0.0063,  ...,  0.0377,  0.0516,  0.0112],\n",
      "        [ 0.1541,  0.3533,  0.0351,  ...,  0.0833,  0.3033, -0.0220],\n",
      "        ...,\n",
      "        [ 0.0659,  0.1611,  0.1938,  ...,  0.1494,  0.2227, -0.0730],\n",
      "        [ 0.0005,  0.1235, -0.0265,  ...,  0.0201,  0.1384,  0.1979],\n",
      "        [ 0.0354,  0.1446,  0.1430,  ...,  0.1763,  0.1053,  0.0076]])\n",
      "[tensor(0.6720), tensor(0.6732), tensor(0.6697), tensor(0.6729), tensor(0.6717), tensor(0.6738), tensor(0.6731), tensor(0.6728)]\n",
      "tensor([[ 0.0902,  0.0697,  0.0567,  ...,  0.4172, -0.0116,  0.0039],\n",
      "        [ 0.0302,  0.0029,  0.2069,  ..., -0.0264,  0.3268,  0.0334],\n",
      "        [ 0.0175,  0.0196,  0.0059,  ...,  0.0103,  0.0163,  0.0169],\n",
      "        ...,\n",
      "        [ 0.0161,  0.0098,  0.0082,  ...,  0.0117,  0.0091,  0.0120],\n",
      "        [ 0.4702, -0.0584,  0.1038,  ...,  0.0598,  0.3008,  0.1470],\n",
      "        [ 0.0143,  0.0329,  0.1210,  ..., -0.0186,  0.2466,  0.0175]])\n",
      "[tensor(0.7432), tensor(0.7425), tensor(0.7427), tensor(0.7416), tensor(0.7442), tensor(0.7435), tensor(0.7415), tensor(0.7430)]\n",
      "tensor([[ 0.0802,  0.0716, -0.0371,  ...,  0.1387,  0.0657,  0.0299],\n",
      "        [ 0.1280, -0.0764,  0.1347,  ...,  0.0821,  0.0151,  0.0510],\n",
      "        [ 0.0418,  0.0066,  0.0245,  ...,  0.0205,  0.0057,  0.0118],\n",
      "        ...,\n",
      "        [ 0.4166,  0.0721,  0.2936,  ...,  0.1597, -0.0244,  0.0506],\n",
      "        [ 0.0248,  0.0878,  0.2087,  ...,  0.0273,  0.2031,  0.0186],\n",
      "        [ 0.2816,  0.1027,  0.0371,  ...,  0.1529,  0.2145,  0.1654]])\n",
      "[tensor(0.7013), tensor(0.7043), tensor(0.7027), tensor(0.7028), tensor(0.7034), tensor(0.7015), tensor(0.7034), tensor(0.7029)]\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cpu\") # operations is in CPU or GPU.\n",
    "M = torch.diag(torch.ones(8)).roll(-1,1) # Create a diagnoise matrix then shift it to the right\n",
    "used_bases = [torch.linalg.matrix_power(M,i).to(DEVICE) for i in range (8)] #\n",
    "\n",
    "N_points = 1000\n",
    "\n",
    "for k_fold in range(3):\n",
    "    classifier = FFNN(8,9) # This indicates that the neural network expects input data with 8 features and will produce output predictions across 9 classes.\n",
    "    encoder = E(8,8)\n",
    "    encoder.load_state_dict(torch.load(f\"encoder_fold_{k_fold+1}.pt\")) # contains the learned parameters (weights and biases) of the encoder model\n",
    "    recovered_points_= torch.load(f\"reference_points_fold_{k_fold+1}.pt\") # These points represent reference points for inference or evaluation in the model\n",
    "    print(recovered_points_)\n",
    "    classifier.load_state_dict(torch.load(f\"classifier_fold_{k_fold+1}.pt\")) # contains the weights and biases learned during training.\n",
    "    classifier.eval() # sets the model to evaluation mode.\n",
    "    encoder.eval() # sets the model to evaluation mode.\n",
    "    # with torch.no_grad():\n",
    "    #     encoder.eval()\n",
    "    #     N_points = 1000\n",
    "    #     rand_idx = np.random.choice(all_X_test.shape[0], N_points)\n",
    "    #     y_tr = encoder(get_tensor(all_X_test[rand_idx]))\n",
    "    #     recovered_points_ = rotate_batch(y_tr,-all_shift_test[rand_idx].flatten(), 6)\n",
    "    #     del y_tr\n",
    "\n",
    "    accuracies_inferred_operator = []\n",
    "    for i in range (-4, 4):\n",
    "        X_test_shift = roll_data(X_test[k_fold], i)\n",
    "        y1 = encoder(get_tensor(X_test_shift))\n",
    "        y_tr_rotated = torch.zeros(X_test_shift.shape[0])\n",
    "        for j, y_ in enumerate(y1):\n",
    "            distances = np.zeros(8)\n",
    "            for d in (range(-4,4)):\n",
    "                x_rotated = used_bases[d].matmul(y_).repeat(N_points,1)\n",
    "                distances[d] = ((x_rotated-recovered_points_)**2).mean(1).topk(2, largest=False)[0].mean()\n",
    "            y_tr_rotated[j] = distances.argmin()\n",
    "        y_tr_est1 = rotate_batch(y1, y_tr_rotated.int(),6)\n",
    "        y_tr1 = classifier(y_tr_est1).argmax(1)\n",
    "        # print(y_tr1- get_tensor(y_test).flatten())\n",
    "        accuracies_inferred_operator.append((1-torch.abs(torch.sign(y_tr1- get_tensor(y_test[k_fold]).flatten()))).mean())\n",
    "\n",
    "    print(accuracies_inferred_operator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
