{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from joblib import dump, load\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from data_loader import CustomSignalData, CustomSignalData1\n",
    "from torch.autograd import Variable\n",
    "from encoder import Encoder as E\n",
    "from helpers import set_cmd_cb, rms_formuula, get_data, get_all_data, get_shift_data, get_operators, plot_cfs_mat, roll_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cpu\")\n",
    "def getFeatureMatrix(rawDataMatrix, windowLength, windowOverlap):\n",
    "    rms = lambda sig: np.sqrt(np.mean(sig**2))\n",
    "    nChannels,nSamples = rawDataMatrix.shape    \n",
    "    I = int(np.floor(nSamples/(windowLength-windowOverlap)))\n",
    "    featMatrix = np.zeros([nChannels, I])\n",
    "    for channel in range(nChannels):\n",
    "        for i in range (I):\n",
    "            wdwStrtIdx=i*(windowLength-windowOverlap)\n",
    "            sigWin = rawDataMatrix[channel][wdwStrtIdx:(wdwStrtIdx+windowLength-1)] \n",
    "            featMatrix[channel, i] = rms(sigWin)\n",
    "    featMatrixData = np.array(featMatrix)\n",
    "    return featMatrixData\n",
    "\n",
    "class FFNN(torch.nn.Module):\n",
    "    def __init__(self, inputSize, outputSize):\n",
    "        super(FFNN, self).__init__()\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(inputSize, 9, bias=False),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "        self.classifer = torch.nn.Sequential(\n",
    "            torch.nn.Linear(9, outputSize, bias=False),\n",
    "            # torch.nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, encoder=None):\n",
    "        if not encoder:\n",
    "            encoder = self.encoder\n",
    "        z = encoder(x)\n",
    "        class_z = self.classifer(z)\n",
    "\n",
    "        return class_z\n",
    "\n",
    "class Operator(nn.Module):\n",
    "    def __init__(self, in_features, n_rotations):\n",
    "        super(Operator, self).__init__()\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          in_features (int): Number of input features which should be equal to xsize.\n",
    "          out_features (out): Number of output features which should be equal to ysize.\n",
    "        \"\"\"\n",
    "        self.in_features = in_features\n",
    "        self.core = torch.nn.Parameter(torch.zeros(3*self.in_features**2)- 0*torch.diag(torch.rand(3*self.in_features**2)/10))\n",
    "        self.core.requires_grad = True\n",
    "        self.n_rotations = n_rotations\n",
    "        \n",
    "    def rotate_batch(self, x, d, out_features):\n",
    "      rotated = torch.empty(x.shape[0], 3*out_features*out_features, device=DEVICE)\n",
    "      phies = [torch.linalg.matrix_power(self.core,i).to(DEVICE) for i in range (0,self.n_rotations+0)]\n",
    "      for i in range (x.shape[0]):\n",
    "        rotated[i] = phies[(d[i]+0)%4].matmul(x[i]) \n",
    "      return rotated\n",
    "\n",
    "    def forward(self, x, d):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          x of shape (batch_size, 3, xsize, xsize): Inputs.\n",
    "        \n",
    "        Returns:\n",
    "          y of shape (batch_size, 3*xsize^2): Outputs.\n",
    "        \"\"\"\n",
    "        z = self.rotate_batch(x, d, self.in_features)\n",
    "        return z\n",
    "def get_tensor(arr):\n",
    "    return torch.tensor(arr, device=DEVICE,dtype=torch.float )\n",
    "\n",
    "def rotate_batch(x, d, out_features):\n",
    "    M = torch.diag(torch.ones(8)).roll(-1,1)\n",
    "    used_bases = [torch.linalg.matrix_power(M,i).to(DEVICE) for i in range (8)]\n",
    "    rotated = torch.empty(x.shape, device=DEVICE)\n",
    "    for i in range (x.shape[0]):\n",
    "        rotated[i] = used_bases[d[i]].matmul(x[i]) \n",
    "    return rotated\n",
    "\n",
    "def clf_acc(model, loader, masks = None, encoder = None):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    iter = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels,_,_ in loader:\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            if masks is not None:\n",
    "                inputs = inputs * masks[:inputs.size()[0]]\n",
    "            labels = labels.to(DEVICE)\n",
    "            labels = labels.flatten()\n",
    "            if encoder:\n",
    "                pred = model(inputs, encoder)\n",
    "            else:\n",
    "                pred = model(inputs)\n",
    "            correct += (1-torch.abs(torch.sign(torch.argmax(pred,dim = 1)- labels))).mean().item()\n",
    "            iter += 1\n",
    "    return correct/iter\n",
    "\n",
    "def compute_accuracy(a, b, loader):\n",
    "    a.eval()\n",
    "    b.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    iter = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs1, inputs2, shift1, shift2, labels, _ in loader:\n",
    "            inputs1 = inputs1.to(DEVICE)\n",
    "            inputs2 = inputs2.to(DEVICE)\n",
    "            shift1 = -shift1.int().flatten().to(DEVICE)\n",
    "            shift2 = -shift2.int().flatten().to(DEVICE)\n",
    "            labels = labels.flatten().to(DEVICE)\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # forward + backward + optimize\n",
    "            y1 = a(inputs1)\n",
    "            y_tr_est1 = rotate_batch(y1,shift1,6)\n",
    "            y_tr1 = b(y_tr_est1)\n",
    "\n",
    "            y2 = a(inputs2)\n",
    "            y_tr_est2 = rotate_batch(y2,shift1,6)\n",
    "            y_tr2 = b(y_tr_est2)\n",
    "\n",
    "            correct += (1-torch.abs(torch.sign(torch.argmax(y_tr1,dim = 1)- labels))).mean().item() + \\\n",
    "                    (1-torch.abs(torch.sign(torch.argmax(y_tr2,dim = 1)- labels))).mean().item()\n",
    "            iter += 1\n",
    "    return correct * 0.5 / iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = '22'\n",
    "\n",
    "Fs = 1000\n",
    "windowLength = int(np.floor(0.1*Fs))  #160ms\n",
    "windowOverlap =  int(np.floor(50/100 * windowLength))\n",
    "\n",
    "X_train = np.zeros([0,8])\n",
    "y_train= np.zeros([0])\n",
    "# X_train = np.zeros([0,8])\n",
    "# y_train = np.zeros([0])\n",
    "for shift in range(0,1): \n",
    "    for files in sorted(os.listdir(f'Subject_{subject}/Shift_{shift}/')):\n",
    "        _, class_,_, rep_ = files.split('_')\n",
    "        if int(class_) in [1,2,3,4,5,6,7,8,9]:\n",
    "            df = pd.read_csv(f'Subject_{subject}/Shift_{shift}/{files}',skiprows=0,sep=' ',header=None)\n",
    "            data_arr = np.stack([np.array(df.T[i::8]).T.flatten().astype('float32') for i in range (8)])\n",
    "            data_arr -= 121\n",
    "            data_arr /= 255.0\n",
    "            feaData = getFeatureMatrix(data_arr, windowLength, windowOverlap)\n",
    "            \n",
    "            if not class_.startswith('9'):\n",
    "                rms_feature = feaData.sum(0)\n",
    "                baseline = 2*rms_feature[-50:].mean()\n",
    "                start_ = np.argmax(rms_feature[::1]>baseline)\n",
    "                end_  = -np.argmax(rms_feature[::-1]>baseline)\n",
    "                feaData = feaData.T[start_:end_]\n",
    "            else:\n",
    "                feaData = feaData.T\n",
    "\n",
    "            # if rep_.startswith('2'):\n",
    "            #     X_test = np.concatenate([X_test,feaData])\n",
    "            #     y_test = np.concatenate([y_test,np.ones_like(feaData)[:,0]*int(class_)-1])\n",
    "            \n",
    "            X_train = np.concatenate([X_train,feaData])\n",
    "            y_train= np.concatenate([y_train,np.ones_like(feaData)[:,0]*int(class_)-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00618173 0.00219444 0.00343597 ... 0.02130139 0.01734183 0.00831423]\n",
      " [0.00623178 0.0024296  0.00369729 ... 0.02960195 0.01487295 0.0117581 ]\n",
      " [0.00609314 0.00215875 0.0031776  ... 0.03120872 0.0161113  0.01154479]\n",
      " ...\n",
      " [0.00525838 0.00124636 0.00275893 ... 0.00294942 0.00233172 0.00322611]\n",
      " [0.00557388 0.0011824  0.00281467 ... 0.00294942 0.00222955 0.00307827]\n",
      " [0.00572891 0.00106079 0.00303145 ... 0.00273894 0.00193672 0.00321169]]\n",
      "(4806, 8)\n",
      "[0. 0. 0. ... 8. 8. 8.]\n",
      "(4806,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train)\n",
    "print(X_train.shape)\n",
    "print(y_train)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_X_train, all_y_train, all_shift_train = get_all_data(X_train, y_train)\n",
    "# all_X_test, all_y_test, all_shift_test = get_all_data(X_test, y_test)\n",
    "\n",
    "all_X1_train, all_X2_train, all_shift_1_train, all_shift_2_train, all_y_shift_train = get_shift_data(all_X_train, all_shift_train, all_y_train)\n",
    "# all_X1_test, all_X2_test, all_shift_1_test, all_shift_2_test, all_y_shift_test = get_shift_data(all_X_test, all_shift_test, all_y_test)\n",
    "\n",
    "# Data loader\n",
    "traindataset = CustomSignalData(get_tensor(X_train), get_tensor(y_train))\n",
    "#testdataset = CustomSignalData(get_tensor(X_test), get_tensor(y_test))\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(traindataset, batch_size = 1, shuffle=True)\n",
    "#testloader = torch.utils.data.DataLoader(testdataset, batch_size=24, shuffle=True)\n",
    "\n",
    "all_train_dataset = CustomSignalData(get_tensor(all_X_train), get_tensor(all_y_train))\n",
    "alltrainloader = torch.utils.data.DataLoader(all_train_dataset, batch_size = 102, shuffle=True)\n",
    "\n",
    "triplet_train_dataset = CustomSignalData1(get_tensor(all_X1_train), get_tensor(all_X2_train), get_tensor(all_shift_1_train), get_tensor(all_shift_2_train), get_tensor(all_y_shift_train))\n",
    "triplettrainloader = torch.utils.data.DataLoader(triplet_train_dataset, batch_size = 102, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eea-10/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['LogisticRegression_limit_data.joblib']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg = LogisticRegression(penalty='l2', C=100).fit(X_train, y_train)\n",
    "dump(reg, 'LogisticRegression_limit_data.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed Forward Neural Network\n",
    "inputDim = 8     # takes variable 'x' \n",
    "outputDim = 9      # takes variable 'y'\n",
    "learningRate = 0.005\n",
    "\n",
    "model = FFNN(inputDim, outputDim)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "crit = torch.nn.CrossEntropyLoss()\n",
    "acc_record = []\n",
    "params_clf = list(model.parameters())# + list(encoder.parameters())\n",
    "optim = torch.optim.Adam(params_clf, lr=learningRate)\n",
    "\n",
    "epochs = 200\n",
    "#encoder = encoder.to(device)\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "\n",
    "    # Converting inputs and labels to Variable\n",
    "    for inputs, labels, _, _ in alltrainloader:\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        labels = labels.long()\n",
    "        labels = labels.flatten()\n",
    "        outputs = model(inputs, None)\n",
    "        optim.zero_grad()\n",
    "        # get loss for the predicted output\n",
    "        losss = crit(outputs, labels) #+ 0.001 * model.l1_regula()\n",
    "        # get gradients w.r.t to parameters\n",
    "        losss.backward()\n",
    "        # update parameters\n",
    "        optim.step()\n",
    "\n",
    "torch.save(model.state_dict(), \"modelwoOperator_limit_data.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-supervised learning model\n",
    "\n",
    "encoder = E(8,8)\n",
    "encoder.to(DEVICE)\n",
    "classifier = FFNN(8,9)\n",
    "classifier.to(DEVICE)\n",
    "\n",
    "parameters = list(encoder.parameters()) + list(classifier.parameters())\n",
    "\n",
    "crit1 = torch.nn.MSELoss()\n",
    "crit2 = torch.nn.CrossEntropyLoss()\n",
    "crit1.to(DEVICE)\n",
    "crit2.to(DEVICE)\n",
    "loss_record = []\n",
    "\n",
    "optimizer = torch.optim.Adam(parameters, lr=0.002)\n",
    "n_epochs = 50\n",
    "\n",
    "for epoch in range(0,n_epochs):\n",
    "    encoder.train()\n",
    "    classifier.train()\n",
    "    for inputs1, inputs2, shift1, shift2, labels, _ in triplettrainloader:\n",
    "        inputs1 = inputs1.to(DEVICE)\n",
    "        inputs2 = inputs2.to(DEVICE)\n",
    "        shift1 = -shift1.int().flatten().to(DEVICE)\n",
    "        shift2 = -shift2.int().flatten().to(DEVICE)\n",
    "        labels = labels.long().flatten().to(DEVICE)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward + backward + optimize\n",
    "        y1 = encoder(inputs1)\n",
    "        y_tr_est1 = rotate_batch(y1,shift1,6)\n",
    "        y_tr1 = classifier(y_tr_est1)\n",
    "\n",
    "\n",
    "        y2 = encoder(inputs2)\n",
    "        y_tr_est2 = rotate_batch(y2,shift2,6)\n",
    "        y_tr2 = classifier(y_tr_est2)\n",
    "\n",
    "        loss =  crit1(y_tr_est1, y_tr_est2) + 0.5*crit2(y_tr1,labels)+ 0.5*crit2(y_tr2,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "torch.save(classifier.state_dict(), \"classifier_limit_data.pt\")\n",
    "torch.save(encoder.state_dict(), \"encoder_limit_data.pt\")\n",
    "with torch.no_grad():\n",
    "    encoder.eval()\n",
    "    N_points = 1000\n",
    "    rand_idx = np.random.choice(all_X_train.shape[0], N_points)\n",
    "    y_tr = encoder(get_tensor(all_X_train[rand_idx]))\n",
    "    recovered_points_ = rotate_batch(y_tr,-all_shift_train[rand_idx].flatten(), 6)\n",
    "    del y_tr\n",
    "\n",
    "torch.save(recovered_points_, \"reference_points_limit_data.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
